---
layout: "post"
title: "VAE"
date: "2022-4-6"
categories: "深度学习"
tags: "DL VAE"
---


## 算法背景及相关工作

&emsp; &ensp; [VAE]属于生成模型。在深度学习时代，比较著名的生成模型主要有两大类GAN和VAE。这里我们主要介绍VAE。VAE的全称为 Variational Auto Encoder，中文名为变分自编码器。


## 算法原理

### 自动编码器（AE）
&ensp; &emsp; 自动编码器属于无监督学习，它包含两部分：Encoder和Decoder。其中Encoder部分将高维的输入信号映射到一个低维特征空间，然后Decoder再根据低维空间特征重建原始数据。由于我们可以从低维空间特征中还原原始数据，因此可以认为通过Encoder编码器得到的低维空间特征可以完全表示原始数据。AE常用来对数据降维。

<center>
    <div>
    <img src=https://cdn.jsdelivr.net/gh/wtyhainan/blog-img@main/VAE/AE.jpg>
    </div>
</center>

&emsp; &ensp; 想象一下，当我们在低维特征空间中随机采样得到 $z^{'}$，然后使用AE中的Decoder对其进行解码，大部分情况下会得到无意义的解码数据。原因在于我们在使用Encoder对数据进行编码操作时没有对数据做任何限制，很难想象可以通过一个随机特征 $z^ {'}$ 解码得到有意义的输出。

************************************

VAE的做法是假设低维特征 $z^i$ 服从先验分布 $p_{\theta}(\mathbf z)$，对原始数据重建则是根据条件分布 $p_{\theta}(\mathbf x \mid \mathbf z)$ 对隐变量 $z^i$ 积分得到：

$$
\begin{equation}
p_{\theta}(\mathbf x) = \int_z p_{\theta} (\mathbf x \mid \mathbf z) \, p_{\theta}(\mathbf z) \, d\mathbf z
\end{equation}

$$

&ensp; &emsp; 然而，上式通常是难以计算的，一方面是先验分布 $p_{\theta}(\mathbf z)$ 是未知的 ***（在后面会知道明明已经对 $p_{\theta}(\mathbf z)$ 的分布做了假设，怎么还未知呢？）***；另一方面，如果分布 ***(哪个分布？)*** 比较复杂，求解上式积分是极其耗时的。为此，引进后验分布 $p_{\theta}(\mathbf x \mid \mathbf z)$，根据贝叶斯公式有：

$$
\begin{equation}
p_{\theta}(\mathbf z \mid \mathbf x) = \frac{p_{\theta}(\mathbf x \mid \mathbf z) \, p_{\theta}(\mathbf z)}{p_{\theta}(\mathbf x)}
\end{equation}
$$

对于VAE，使用Encoder来拟合 $p_{\theta}(\mathbf z\mid \mathbf x)$，记为$q_{\phi}(\mathbf z \mid \mathbf x)$。 ***Decoder则用来拟合 $p_{\theta}(\mathbf x \mid \mathbf z)$，记为 $q_{\phi}(\mathbf x \mid \mathbf z)$ <----- 这里貌似有点问题，应该是拟合整个积分吧？***。 对于后验估计 $q_{\phi}(\mathbf z \mid \mathbf x)$，我们希望它接近真实后验分布 $p_{\theta}(\mathbf z \mid \mathbf x)$，可通过KL散度衡量两个概率分布之间的距离，有：


$$
\begin{equation}
\begin{aligned}

D_{KL} \Big( q_{\phi} (\mathbf z \mid \mathbf x) \mid \mid p_{\theta}(\mathbf z \mid \mathbf x) \Big) &= \int q_{\phi}(\mathbf z \mid \mathbf x) \log \frac{q_{\phi}(\mathbf z \mid \mathbf x)}{p_{\theta}(\mathbf z \mid \mathbf x)} d \mathbf z \\

&= \int q_{\phi}(\mathbf z \mid \mathbf x) \log \frac{q_{\phi}(\mathbf z \mid \mathbf x) p_{\theta}(\mathbf x)}{p_{\theta}(\mathbf x, \mathbf z)} \, d \mathbf z \\

&= \int q_{\phi} (\mathbf z \mid \mathbf x) \log p_{\theta}(\mathbf x) \, d\mathbf z + \int q_{\phi}(\mathbf z \mid \mathbf x) \log \frac{q_{\phi}(\mathbf z \mid \mathbf x)}{p_{\theta}(\mathbf x, \mathbf z)} \, d \mathbf z \\
&= \log p_{\theta}(\mathbf x) + \int q_{\phi}(\mathbf z \mid \mathbf x) \log \frac{q_{\phi}(\mathbf z \mid \mathbf x)}{p_{\theta}(\mathbf x \mid \mathbf z) p_{\theta}(\mathbf z)} \, d \mathbf z \\

&= \log p_{\theta}(\mathbf x) + \int q_{\phi}(\mathbf z \mid \mathbf x) \log \frac{q_{\phi}(\mathbf z \mid \mathbf x)}{p_{\theta}(\mathbf z)} \, d \mathbf z - \int q_{\phi}(\mathbf x \mid \mathbf z) \log p_{\theta}(\mathbf x \mid \mathbf z) \, d \mathbf z \\

&= \log p_{\theta}(\mathbf x) + D_{KL}\Big( q_{\phi}(\mathbf z \mid \mathbf x) \mid \mid p_{\theta}(\mathbf z) \Big) - E_{\mathbf z \sim q_{\phi}(\mathbf z \mid \mathbf x)}\Big( \log p_{\theta}(\mathbf x \mid \mathbf z)\Big)

\end{aligned}
\end{equation}
$$

因此有：

$$
\begin{equation}
\log p_{\theta}(\mathbf x) - D_{KL} \Big(q_{\phi}(\mathbf z \mid \mathbf x) \mid \mid p_{\theta}(\mathbf z \mid \mathbf x) \Big) =E_{\mathbf z \sim q_{\phi}(\mathbf z \mid \mathbf x)}\Big( \log p_{\theta}(\mathbf x \mid \mathbf z) \Big) - D_{KL} \Big(q_{\phi}(\mathbf z \mid \mathbf x) \mid \mid p_{\theta}(\mathbf z) \Big)
\end{equation}
$$

&ensp; &emsp; $\log p_{\theta}(\mathbf x)$ 表示生成数据的对数似然。$D_{KL} \Big( q_{\phi}(\mathbf z \mid \mathbf x) \mid \mid p_{\theta}(\mathbf z \mid \mathbf x)\Big)$ 表示Encoder输出的后验分布 $q_{\phi}(\mathbf z \mid \mathbf x)$与真实分布 $p_{\theta}(\mathbf z \mid \mathbf x)$ 之间的距离。对于生成模型，我们希望最大化 $\log p_{\theta}(\mathbf x)$ 同时最小化 $D_{KL} \Big( q_{\phi}(\mathbf z \mid \mathbf x) \mid \mid p_{\theta}(\mathbf z \mid \mathbf x)\Big)$ 。上式的右边称为 Evidence lower bound，简称ELBO。由于KL散度的非负性，基于（4）式可得出：

$$
\begin{equation}
\log p_{\theta}(\mathbf x) \ge E_{\mathbf z \sim q_{\phi}(\mathbf z \mid \mathbf x)} \Big( \log p_{\theta}(\mathbf x \mid \mathbf z) \Big) - D_{KL}\Big( q_{\phi}(\mathbf z \mid \mathbf x) \mid \mid p_{\theta}(\mathbf z) \Big) 
\end{equation}
$$

对于VAE，其优化的目标函数可写为：

$$\mathcal L(\theta, \phi) = -E_{\mathbf z \sim q_{\phi}(\mathbf z \mid \mathbf x)}\Big( p_{\theta}(\mathbf x \mid \mathbf z) \Big) + D_{KL}\Big( q_{\phi}(\mathbf z \mid \mathbf x) \mid \mid p_{\theta}(\mathbf z) \Big)$$

$$
\begin{equation}
\theta ^{*}, \phi ^{*} = \mathop {argmin} \limits_{\theta, \phi} \mathcal L(\theta, \phi) 
\end{equation}
$$

&emsp; &ensp; 为了优化（6）式，需要对 $p_{\theta}(\mathbf z)$ 及 $q_{\phi}(\mathbf z \mid \mathbf x)$ 做一定的假设：

$$
\begin{equation}
\begin{aligned}
q_{\phi}(\mathbf z \mid \mathbf x^i) &= \mathcal N(\mathbf z; \mu^{(i)}, \sigma^{2(i)}I) \\
p_{\theta}(\mathbf z) &= \mathcal N(\mathbf z, 0, I)    
\end{aligned}
\end{equation}
$$

上面两个假设相当于认为隐变量 $\mathbf z$相互独立，我们希望通过Encoder拟合先验分布 $p_{\theta}(\mathbf z)$ ***（可否理解为，我们人为的给Encoder指定了一个前进的方向）***。由于我们预先对 $q_{\phi}(\mathbf z \mid \mathbf x)$ 和 $p_{\theta}(\mathbf z)$ 的分布做了假设，因此Encoder只要输出 $q_{\phi}(\mathbf z \mid \mathbf x)$分布的参数即可，在这里也就是 $\mu$ 和 $\sigma$。

&emsp; &ensp; 在（6）式中的 $E_{\mathbf z \sim q_{\phi}(\mathbf z \mid \mathbf x)}\Big( \log p_{\theta}(\mathbf x \mid \mathbf z) \Big)$ 表示了Decoder的重构误差。当我们从分布 $q_{\phi}(\mathbf z \mid \mathbf x)$ 中抽样得到隐变量 $\mathbf z^i$ 后，我们希望Decoder根据 $z^i$ 得到原始数据的概率分布 $\log p_{\theta}(\mathbf x \mid \mathbf z)$，使用Decoder来完成，因此我们需要最大化Decoder输出的对数似然。可通过Monte Carlo方法来计算重构误差 ***(VAE作者认为只要训练的batchsize足够大，只取一个样本用于近似也是可以的)***：

$$
\begin{equation}
-E_{\mathbf z \sim q_{\phi}(\mathbf z \mid \mathbf x)}\Big( \log p_{\theta}(\mathbf x^i \mid \mathbf z) \Big) \approx-\frac{1}{L} \sum_{l=1}^L \Big( \log p_{\theta}(\mathbf x ^i \mid \mathbf z^{i, l}) \Big)
\end{equation}
$$

为了避开从 $q_{\phi}(\mathbf z \mid \mathbf x)$ 中采样无法计算梯度的问题，VAE采用了重采样技术。做法就是随机从标准正态分布随机采样一个样本，然后乘以Encoder预测的标准差，再加上Encoder预测的均值，这样就能计算该损失对Encoder网络参数的梯度了。

&emsp;&ensp; 根据建模的类型， $p_{\theta}(\mathbf x \mid \mathbf z)$ 可以是高斯分布或者伯努利分布。对于高斯分布，假定 $p_{\theta}(\mathbf x \mid \mathbf z)$属于各分量相互独立的多元高斯分布：$p_{\theta}(\mathbf x \mid \mathbf z) = \mathcal N(\mathbf x; \mu, \sigma^2 I)$。通常情况，往往假定这个多元高斯分布的标准差为一个常量，而均值则由Decoder给出，因此重构误差为：

$$
\begin{equation}
-\log p_{\theta}(\mathbf x^i \mid \mathbf z^{i, l}) = C_1 + C_2 \sum_{d}^D(x_d^{(i)} - (f_{\theta}(\mathbf z^{(i, l)}))_d)^2
\end{equation}
$$

因此，总的损失函数可写为：

$$
\begin{equation}
\begin{aligned}
\mathcal L(\theta, \phi) &= -E_{z \sim q_{\phi}(\mathcal z \mid \mathcal x)} \Big( \log p_{\theta}(\mathcal x \mid \mathbf z) \Big) + D_{KL} \Big( q_{\phi}(\mathbf z \mid \mathbf x) \mid \mid p_{\theta}(\mathbf z)\Big) \\
 & \approx -\frac{1}{L} \sum_{l=1}^L \Big( \log p_{\theta}(\mathbf x \mid \mathbf z^l)\Big) + D_{KL} \Big( q_{\phi}(\mathbf z \mid \mathbf x)  \mid \mid p_{\theta}(\mathbf z) \Big) \\
 &= \frac{C}{L} \sum_{l=1}^L \sum_{d}^D \Big( x_d - (f_{\theta}(\mathbf z^l))_d \Big)^2 + \frac{1}{2} \sum_{j=0}^n \Big( (\sigma_j)^2 + (\mu_j)^2 - 1 - \log((\sigma_j)^2)\Big)
\end{aligned}
\end{equation}
$$

## 算法优缺点（这个是比较难的）

### 优点:
&emsp; &ensp; 1）VAE属于生成模型，当模型训练好后，可通过 $q_{\phi}(\mathbf z \mid \mathbf x)$ 采样得到隐变量 $z^i$，然后送入Decoder生成数据。也就是我们可以根据模型产生许多与训练数据同分布的数据，放我们在遇到训练数据不足时，可采取这个办法来生成许多可用的伪数据

### 缺点:
&emsp; &ensp; 1）使用KL散度来衡量概率分布的距离，而KL散度并不是一个对称的距离测度；

&ensp; &emsp; 2）在解码时假设 $p_{\theta}(\mathbf x \mid \mathbf z)$ 属于各变量相互独立的多元高斯分布，然而当输入X为图片数据时，各个像素之间并不是独立的关系，每个像素均与该像素邻域产生一定的联系。

&emsp; &ensp; 3) 知乎上许多网友均表示VAE生成的图片比较模糊。
## 总结

&ensp; &emsp; 考虑如下问题，假设我们有一个独立同分布的数据集 $ \{ \mathbf x_i \}_{i=1}^N$，我们的目标是根据这个数据集估计其概率分布 $p_{\theta}(\mathbf x)$。假设存在一个服从先验分布 $p_{\theta}(\mathbf z)$ 的隐变量 $\mathbf z$，数据 $\{ \mathbf x_i\}_{i=1}^N$ 是通过隐变量 $\mathbf z$ 经过条件分布 $p_{\theta}(\mathbf x \mid \mathbf z)$ 得到。因此，条件概率分布 $p_{\theta}(\mathbf x \mid \mathbf z)$ 可通过最大化 $\log p_{\theta}(\mathbf x)$ 得到。

&emsp; &ensp; 然而，由于 $p_{\theta}(\mathbf x \mid \mathbf z)$ 可能过于复杂或者积分不容易计算等原因，直接通过最大化 $\log p_{\theta}(\mathbf x)$ 来求 $p_{\theta}(\mathbf x \mid \mathbf z)$ 的办法并不可取。VAE采取了一种迂回的办法来解决这个问题。VAE引入一个后验分布 $p_{\theta}(\mathbf z \mid \mathbf x)$，该后验分布不可知。利用Encoder来拟合这个后验分布，再对先验分布 $p_{\theta}(\mathbf z)$ 及条件分布 $p_{\theta}(\mathbf x \mid \mathbf z)$ 做出一定的假设后，通过最大化ELBO来得到 $p_{\theta}(\mathbf x \mid \mathbf z)$。VAE的主要目的是得到Decoder，为了达到这个目的VAE对隐变量分布做了简单假设，并且引入Encoder。神奇的是Encoder的引入竟是为了辅助训练Decoder，隐变量的分布可以假设成各分量相互独立的多元高斯分布。

<center>
<div>
<img src=https://cdn.jsdelivr.net/gh/wtyhainan/blog-img@main/VAE/VAE.jpg>
</div>
</center>


### 参考资料

[生成模型之VAE](https://zhuanlan.zhihu.com/p/452743042) 

***注：文中公式基本来自上述博客，如想了解更详细的推导建议查看原帖。***

[VAE]: https://arxiv.org/pdf/1312.6114.pdf

